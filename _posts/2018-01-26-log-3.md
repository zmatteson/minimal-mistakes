---
title: "Choose your own appventure, part 3"
categories:
  - appventure
  - log
tags:
  - keras
  - tensorflow
  - cyoa
---

I have a little bit of catching up to do with the state of my project.  I created my dataset, and trained it on the Keras model, and wasn't super happy with the results. 

Before I show a small batch, first I want to define a few terms, taken mostly from the Keras.io [documentation](https://keras.io/getting-started/faq/).

 * Epoch: an arbitrary cutoff, generally defined as "one pass over the entire dataset", used to separate training into distinct phases, which is useful for logging and periodic evaluation.
 * Diversity: A way to describe the temperature, which is a hyperparameter (a term from Bayesian statistics) that determines the randomness of the output.
* Seed: a sample from the training data that the model uses to predict the next words

These samples are all generated from the same seed (chosen randomly): "at he could send into the history of the".

Diversity of 0.2: 

>at he could send into the history of the soul of the sensation of the sense, and it is not the self sacrificed to the same the more the same the sense, and as the sense, and as the world of the more the sensation of the principally and some consideration of the proterration, as the world, and a pression of the self respecting that it is a superiority and the observation of the soul of the consideration to the self experience as the prin

So, here our output is mostly intelligible! It rambles on a bit and doesn't make any sense, but most of the words aren't misspelled.  But the problem with this is that it's simply too boring.  Let's look at a higher diversity, which could give us more interesting results.

Diversity of 1.0:

>at he could send into the history of the "sagn greater sulsish that all their will not been a
dangly,
and readily wea. the more have wish to whicd moralle--or effect.s and they insciation, far is previcile and
origin, the
men which was erranne and doudls, evorwazity, the cruely. he had has love of
life. in his hearted
immoral-courtism, and the permittent for instance, which
we dourty--a long will procount,
but, as to sair to a certain t

This is more exciting, but suffers from curious indentation, and more made-up words.  There are a couple gems that almost work: 'cruely, he had has love of life', 'in his hearted immoral-courtism', but generally, it's nonesense.

Let's look at the highest diversity sample of 1.2: 

>at he could send into the history of the -clongs of nature
freeicates to theative, thus still
his vanity hardeminies a praisos of mechologic feels them--we did not
federecision of dread) tought the older for a semmnle affair--one
unciacce to posities accro. tooe-time-craws into suffe; fis; and honest. theroy, stupilization. fundance and indust one everyther out--he every witchingmenn, in him, many by a doc light of all inflict. we even

So, I like this one the most, mainly because of the phrase "he every witchingmenn", which sounds like really cool Old English, but still the vibe is off from a Choose your own adventure book. It seems almost Shakesperian at times, with a surplus of e's and thus's.  

I decided to ditch Keras and branch out into other github RNNs available.  The classic text generation network is char-rnn, developed by Andrej Karpathy, formerly a Phd student at Stanford and research scientist at OpenAI. In [this](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) fantastic blog post, The Unreasonable Effectiveness of Recurrent Neural Networks, Karpathy explains his code in greater detail.

Reed Morgan Milewicz used char-rnn to create a series of Magic the Gathering Cards, showcased in an interesting [Vice article](https://motherboard.vice.com/en_us/article/bmjke3/the-ai-that-learned-magic-the-gathering).  I liked his results, so I cloned a Tensorflow version of char-rnn, called char-rnn-tensorflow, https://github.com/sherjilozair/char-rnn-tensorflow, to use for the text generation.

Last night, I trained a model using default settings on my dataset, so let's dive into my results:

>"Wells, speaking three out," you say. "I can sex
River, relicolual will level for
upposed muchous to feel? In no inspeditys side,” Matt
says later new where
home no
villa."
Zokil lak to the pyramides of escape where you liest

So we might want to turn the temperature down, but I like this output a lot more.  It's already more entertaining than the Keras model, although neither makes much sense.

>rustan moving itsulf lymatake exhaushed on the Cognar
the dab of your chand!
Turn to page 29.89
"Ready dever-fill rapidly on again therk?" you
remzete and slip in this please.
For a change and supplat, your head lie laheld to feel around
you bare.

Here we see "Turn to page 29.89" slipping in, which is actually a feature from COYA books that I need to scrub from my training data. 

>Thank or shore, you will move to think hourted to fighter,
tnerth your begned. "Yes?"
The End Prarana, was expedie-crevissas."
The thas cus side realized into the rivers startleful that if you will be stay is a
trapture of also that is
hand headquartle made agong up. At ago butering, holds slowly, two
large war Fire
or voice factures has been, a
few must all screen will force off. Even they'
s over boulders.
"Zokive arrive!" she skid sausing one of the cockl

So, for initial results, I'm not unhappy, but clearly we have a LONG way to go towards generating a coherent story.  In fact, I'm not entirely sure how possible that will be.  But my goal ins't to create a fully functional Choose Your Own Adventure clone - it's to teach myself a little bit more about using deep learning in an app that's going into production.

I will spend a few more days tinkering with the training parameters, and then move forward to get this thing deployed somewhere on a VM. My next blog post will focus on the understanding the code - I'll dive into the details of how RNNs work, and how I can improve my output.