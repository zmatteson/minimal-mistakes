---
title: "Choose your own appventure, part 4"
categories:
  - appventure
  - log
tags:
  - keras
  - tensorflow
  - cyoa
  - goals
---

I'm going to start trying to summarize my weekly goals for my app on Mondays.  Or maybe Sunday nights.  Whatever progress I've made towards that goal will be wrapped up over the weekend, and my results will be compared to my expectations.  Let's call this week 0.

Expectation: working Tensorflow model by the end of week 0. 

DISCLAIMER: I'm new to python and tensorflow, so I'm going to overanalyze the char-rnn-tensorflow code and make sure I understand even the smallest, trivial things.  When I write a formal walkthrough, I'll filter through my notes and pull out the information that I think would be helpful for the larger public.

Let's start by going under the hood of our Tensorflow code. Inside char-rnn-tensorflow, we have some markdown files and configuration files, which we will ignore, a data folder and logs folder, and a save folder.  Data holds our corpus of text, which is required to be called input.txt, so different inputs are organized by folder.  Logs stores TensorFlow event files, organized by date.  We'll make a note to go back through and piece together what these log files are and how to read them.  I'm pretty sure it's used for TensorBoard, which is TF's Visualization Toolkit.  I don't have TensorBoard installed on my Linux rig, but it won't be hard to loop around and get it running.

Save is an interesting folder - it is where the TF models are stored.  Models have .data, .index, and .meta files associated with them.  I'll look at that this folder in greater detail on a different day.  

The remaining files are model.py, sample.py, train.py and utils.py.  I'm going to start with train.py, which is the script we ran to train our model (is that too obvious?)

1) Imports

First looking at the imports, we have `from __future__ import print_function` which means we're dealing with python written in 2.7, because we are importing the print function from python3. I actually had no idea you could do that in python, so that's kind of fascinating.

`import tensorflow as tf`

This should be self explanatory.

`import argparse`

This module will help parse CLI arguments

`import time`

I often wish I could do the same thing with my life.

`import os`

This will let us deal with the filesystem.

`from six.moves import cPickle`

six.moves lets python2 import modules from python that are renamed.  Seems confusing as hell.  cPickle is the faster version of the Pickle module (implemented in C) which supports object serialization.  Learned a few new things about python by studying these imports.  Actually a little surprised and happy I spent time on something that I normally would glance at.

`from utils import TextLoader`
`from model import Model`

These imports are from model.py and utils.py, which I will look at later.

2) main()

First we create the argument parser and define a few different options.  Let's look at each possible argument we can run.

`--data_dir` will let us point our model to our data, defaulting to some data that the model provides. Note to self: fork this project and clean up the default data.

`--save_dir` will let us specify the save directory. Maybe useful for dealing with different models and training sets?

`--log_dir` will let us specify the directory to store tensorboard logs

`--rnn_size` determines the size of the Recurrent Neural Network hidden state.  I'm going to paraphrase Denny Britz's [article](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) to unpack this one. The recurrence of an RNN invovles repeating the same task on every element of a sequence.  This takes place in a loop that contains a hidden state, which acts as the memory.  At this point, I'm just going to leave myself a reading list so I can dive into what's actually going on at a later point - but I'm just going to blindly assume that increasing this number will cause the RNN to lose less information from an input stream over time. Default is 128. 

READING LIST:  

https://www.cs.toronto.edu/~fritz/absps/RNN13.pdf
http://suriyadeepan.github.io/2017-01-07-unfolding-rnn/
http://www.deeplearningbook.org/

`--num_layers` is the number of layers in the RNN - default of 2.  This particular implementation of an RNN is actually a LSTM : or Long Short Term Memory network. My initial impression of this feature is that each layer increases depth of each cell - which will let our network remember more information.  

ADD ON TO THE READING LIST:
http://colah.github.io/posts/2015-08-Understanding-LSTMs/

Both increasing the RNN size and number of layers will definitely let us tweak our output - but its not immediately obvious what this effect will be.  I'm feeling sleepy, so I'm going to stop here - but as an aside, I'll go ahead and increase my layers to 4 and train the model again overnight.  